{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to test possible solutions to https://github.com/microsoft/LightGBM/issues/3713."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, wait\n",
    "\n",
    "from lightgbm.dask import DaskLGBMRegressor, DaskLGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cluster with 3 workers. Since this is a `LocalCluster`, those workers are just 3 local processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 3\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "print(f\"View the dashboard: {cluster.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
      "\u001b[K     |████████████████████████████████| 279 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest) (20.4)\n",
      "Collecting pluggy<1.0.0a1,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest) (20.2.0)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging->pytest) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->pytest) (2.4.7)\n",
      "Installing collected packages: pluggy, iniconfig, toml, py, pytest\n",
      "Successfully installed iniconfig-1.1.1 pluggy-0.13.1 py-1.10.0 pytest-6.2.1 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the link above to view a diagnostic dashboard while you run the training code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from dask.array.utils import assert_eq\n",
    "from distributed.utils_test import client, cluster_fixture, gen_cluster, loop\n",
    "from sklearn.datasets import make_blobs, make_regression\n",
    "\n",
    "import lightgbm\n",
    "import lightgbm.dask as dlgbm\n",
    "\n",
    "data_output = ['array', 'scipy_csr_matrix', 'dataframe']\n",
    "data_centers = [[[-4, -4], [4, 4]], [[-4, -4], [4, 4], [-4, 4]]]\n",
    "\n",
    "\n",
    "def _create_data(objective, n_samples=100, centers=2, output='array', chunk_size=50):\n",
    "    if objective == 'classification':\n",
    "        X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=42)\n",
    "    elif objective == 'regression':\n",
    "        X, y = make_regression(n_samples=n_samples, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(objective)\n",
    "    rnd = np.random.RandomState(42)\n",
    "    weights = rnd.random(X.shape[0]) * 0.01\n",
    "\n",
    "    if output == 'array':\n",
    "        dX = da.from_array(X, (chunk_size, X.shape[1]))\n",
    "        dy = da.from_array(y, chunk_size)\n",
    "        dw = da.from_array(weights, chunk_size)\n",
    "    elif output == 'dataframe':\n",
    "        X_df = pd.DataFrame(X, columns=['feature_%d' % i for i in range(X.shape[1])])\n",
    "        y_df = pd.Series(y, name='target')\n",
    "        dX = dd.from_pandas(X_df, chunksize=chunk_size)\n",
    "        dy = dd.from_pandas(y_df, chunksize=chunk_size)\n",
    "        dw = dd.from_array(weights, chunksize=chunk_size)\n",
    "    elif output == 'scipy_csr_matrix':\n",
    "        dX = da.from_array(X, chunks=(chunk_size, X.shape[1])).map_blocks(scipy.sparse.csr_matrix)\n",
    "        dy = da.from_array(y, chunks=chunk_size)\n",
    "        dw = da.from_array(weights, chunk_size)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown output type %s\" % output)\n",
    "\n",
    "    return X, y, weights, dX, dy, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the Dask Arrays `data` and `labels` are lazy. Before training, you can force the cluster to compute them by running `.persist()` and then wait for that computation to finish by `wait()`-ing on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w, dX, dy, dw = _create_data(\n",
    "    'classification',\n",
    "    output=\"array\",\n",
    "    centers=data_centers[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data set up on the workers, train a model. `lightgbm.dask.DaskLGBMRegressor` has an interface that tries to stay as close as possible to the non-Dask scikit-learn interface to LightGBM (`lightgbm.sklearn.LGBMRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_reg = DaskLGBMRegressor(\n",
    "    random_state=708,\n",
    "    objective=\"regression_l2\",\n",
    "    tree_learner=\"data\",\n",
    "    n_estimators=10\n",
    ")\n",
    "\n",
    "dask_reg.fit(\n",
    "    client=client,\n",
    "    X=dX,\n",
    "    y=dy,\n",
    ")\n",
    "\n",
    "# predictions asking for predcontrib should add\n",
    "# the contribution column\n",
    "preds = dask_reg.predict(\n",
    "    X,\n",
    "    raw_score=True\n",
    ").compute()\n",
    "preds_with_contrib = dask_reg.predict(\n",
    "    data[:1000, :],\n",
    "    pred_contrib=True,\n",
    "    raw_score=True\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "local_reg = LGBMRegressor(\n",
    "    random_state=708,\n",
    "    objective=\"regression_l2\",\n",
    "    tree_learner=\"data\",\n",
    "    n_estimators=10\n",
    ")\n",
    "\n",
    "local_reg.fit(\n",
    "    X=data.compute(),\n",
    "    y=reg_target.compute(),\n",
    ")\n",
    "\n",
    "# predictions asking for predcontrib should add\n",
    "# the contribution column\n",
    "local_preds = local_reg.predict(\n",
    "    data.compute()[:1000, :],\n",
    "    raw_score=True\n",
    ")\n",
    "local_preds_with_contrib = local_reg.predict(\n",
    "    data.compute()[:1000, :],\n",
    "    pred_contrib=True,\n",
    "    raw_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901,\n",
       " 0.5005821951251901]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[100] for x in preds_with_contrib[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606,\n",
       " 0.500582195028606]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[100] for x in local_preds_with_contrib[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-0ad8e621734d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_eq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0massert_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_with_contrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_preds_with_contrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/array/utils.py\u001b[0m in \u001b[0;36massert_eq\u001b[0;34m(a, b, check_shape, check_graph, check_meta, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_computed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_computed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dask.array.utils import assert_eq\n",
    "\n",
    "assert_eq(preds_with_contrib, local_preds_with_contrib, atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model produced by this training run is an instance of `DaskLGBMRegressor`. To get a regular non-Dask model (which can be pickled and saved), run `.to_local()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = dask_reg.to_local()\n",
    "type(local_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize this model by looking at a data frame representation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.booster_.trees_to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
